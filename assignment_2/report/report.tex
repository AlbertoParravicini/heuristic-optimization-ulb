\documentclass[
12pt,
a4paper,
oneside,
headinclude,
footinclude]{article}

\usepackage[table,xcdraw,svgnames, dvipsnames]{xcolor}
\usepackage[capposition=bottom]{floatrow}
\usepackage[colorlinks]{hyperref} % to add hyperlinks
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{csquotes}
\usepackage{amsmath} % For the big bracket
\usepackage[export]{adjustbox}[2011/08/13]
% \usepackage{subfig}
\usepackage{array}
\usepackage{url}
\usepackage{graphicx} % to insert images
\usepackage{titlepic} % to insert image on front page
\usepackage{geometry} % to define margin
\usepackage{listings} % to add code
\usepackage{caption}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage{color}

\usepackage[nochapters, dottedtoc]{classicthesis}
\usepackage{listings} % For Python code
\usepackage{float}
\usepackage[caption = false]{subfig} % For 2x2 grid of images


\usepackage[ruled]{algorithm2e} % For pseudo-code

\usepackage{mathpazo}

\usepackage{amsthm} % For definitions and theorems

\theoremstyle{definition} % Define the style of definitions
\newtheorem{definition}{Definition}[section]


\usepackage{lipsum} % For testing
\usepackage{color}

\usepackage{etoolbox}

\usepackage{bm} % For bold math

\usepackage{setspace}
\usepackage{minted}

% For tables
\usepackage{amssymb}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\definecolor{webbrown}{rgb}{.6,0,0}

\usepackage{titlesec} % to customize titles
\titleformat{\chapter}{\normalfont\huge}{\textbf{\thechapter.}}{20pt}{\huge\textbf}[\vspace{2ex}\titlerule] % to customize chapter title aspect
\titleformat{\section} % to customize section titles
{\fontsize{14}{15}\bfseries}{\thesection}{1em}{}

\titlespacing*{\chapter}{0pt}{-50pt}{20pt} % to customize chapter title space

\graphicspath{ {./figures/} } % images folder
\parindent0pt \parskip10pt % make block paragraphs
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm,headheight=3cm,headsep=3cm,footskip=1cm} % define margin
\hyphenation{Fortran hy-phen-ation}

\AtBeginDocument{%
	\hypersetup{
		colorlinks=true, breaklinks=true, bookmarks=true,
		urlcolor=webbrown, citecolor=Black, linkcolor=Black% Link colors
}}


\pagestyle{plain}
\title{\textbf{Heuristic Optimization \\ Implementation Exercise 2}}
\author{{Alberto Parravicini}}
\date{}	% default \today



% ====================================================
% =============================================== BEGIN
% ====================================================


\begin{document}

\maketitle
\pagenumbering{roman}
\setcounter{page}{1}





% ====================================================
% =============================================== COMPILATION
% ====================================================

\section{How to compile and run the code}
\subsection{\textbf{Prerequisites}}
\begin{itemize}
    \item \textbf{Cmake}, \textbf{make}, a \textbf{C++11} compiler.
    \item \textbf{Armadillo}, a linear algebra library. How to install it:\\
        \-\quad\texttt{sudo apt-get install liblapack-dev}\\
        \-\quad\texttt{sudo apt-get install libblas-dev}\\
        \-\quad\texttt{sudo apt-get install libboost-dev}\\
        
        \quad\texttt{sudo apt-get install libarmadillo-dev}
\end{itemize}
\subsection{\textbf{Compilation}}
\begin{itemize}
   \item Move to the \textit{build} folder (delete its content if it's not empty)\\
    \-\quad\texttt{cd build}
   \item Run \textit{cmake}\\
    \-\quad\texttt{cmake ../src}
   \item Run \textit{make}\\
    \-\quad\texttt{make}
\end{itemize}

The executables will be located in the \textit{build} folder, and are called \textbf{flowshop\_iga} (for \textbf{Iterated Greedy}) and \textbf{flowshop\_gen} (for the \textbf{Memetic Algorithm}).

\newpage
\subsection{\textbf{How to run the programs}}
The generic syntax to run the \textbf{Iterated Greedy Algorithm} is\\
\-\quad\texttt{./flowshop\_iga} \\
\-\quad\quad\texttt{-{}-filename ../instances/instance\_name} \\
\-\quad\quad\texttt{-{}-random\_seed 42} \\
\-\quad\quad\texttt{-{}-distr\_vec\_size 6} \\
\-\quad\quad\texttt{-{}-lambda 2} \\
\-\quad\quad\texttt{-{}-max\_time [msec]} \\
\-\quad\quad\texttt{-{}-write\_exec\_trace [0|1]} \\


\begin{itemize}
    \item \texttt{-{}-filename}, \texttt{-f}: path to the instance file to be used.
    \item \texttt{-{}-random\_seed}, \texttt{-r}: integer number, used as seed for random number generation by the program. If omitted, the seed is randomized.
    \item \texttt{-{}-distr\_vec\_size}, \texttt{-d}: how many elements in the candidate solutions should be considered in the \textit{Destruction/Construction} procedure. Should be a value between $0$ and the number of jobs ($3$ is the default).
    \item \texttt{-{}-lambda}, \texttt{-l}: parameter that influences the temperature of the algorithm. Lower values of $\lambda$ will slow the convergence. 
    \item \texttt{-{}-max\_time}, \texttt{-t}: maximum amount of time for which the algorithm should run. It is expressed in \textit{milliseconds}.
    By default it runs for $300$ seconds on $50$ jobs instances and for $5000$ seconds on $100$ jobs instances.
    \item \texttt{-{}-write\_exec\_trace}, \texttt{-e}: if $1$, the execution trace of the algorithm (i.e. the best current results at a given amount of time) is written to the file \textit{./results/results\_details\_iga.csv} (default $0$).
\end{itemize}

Example:\\
\quad\texttt{./flowshop\_iga -f ../instances/50\_20\_10 -l 2 -d 5}

\textbf{IGA} will solve the instance \textbf{50\_20\_10} using \textit{$\lambda = 2$}, with a random seed, maximum time of $300$ seconds, and \textit{destruction vector size} of $5$; no execution trace is written.


\newpage
The generic syntax to run the \textbf{Memetic Algorithm} is\\
\-\quad\texttt{./flowshop\_iga} \\
\-\quad\quad\texttt{-{}-filename ../instances/instance\_name} \\
\-\quad\quad\texttt{-{}-random\_seed 42} \\
\-\quad\quad\texttt{-{}-population\_size 10} \\
\-\quad\quad\texttt{-{}-crossover\_prob 0.99} \\
\-\quad\quad\texttt{-{}-mutation\_prob 0.05} \\
\-\quad\quad\texttt{-{}-weights\_type [uni|uni10|sm]} \\
\-\quad\quad\texttt{-{}-mutation\-type [tr]} \\
\-\quad\quad\texttt{-{}-max\_time [msec]} \\
\-\quad\quad\texttt{-{}-write\_exec\_trace [0|1]} \\


\begin{itemize}
    \item \texttt{-{}-filename}, \texttt{-f}: path to the instance file to be used.
    \item \texttt{-{}-random\_seed}, \texttt{-r}: integer number, used as seed for random number generation by the program. If omitted, the seed is randomized.
    \item \texttt{-{}-population\_size}, \texttt{-p}: positive integer, it controls the population size of the algorithm (default $10$).
    \item \texttt{-{}-crossover\_prob}, \texttt{-c}: value between 0 and 1. It controls the probability that two candidate solutions are combined together instead of just copying them (default $0.99$).
    \item \texttt{-{}-mutation\_prob}, \texttt{-m}: value between 0 and 1. It controls the probability that a candidate solution is randomly mutated (default $0.05$)).
    \item \texttt{-{}-weights\_type}, \texttt{-w}: it controls the probability that each candidate solution is selected by the algorithm (default \texttt{sm}, \textit{Softmax}).
    \item \texttt{-{}-mutation\-type}, \texttt{-n}: it controls the type of mutation done by the algorithm (default \texttt{tr}, \textit{Transpose}).
    \item \texttt{-{}-max\_time}, \texttt{-t}: same as \textbf{IGA}.
    \item \texttt{-{}-write\_exec\_trace}, \texttt{-e}: same as \textbf{IGA}. Results are written to the file \textit{./results/results\_details\_gen.csv} (default $0$).
\end{itemize}

Example:\\
\quad\texttt{./flowshop\_gen -f ../instances/50\_20\_10 -p 20 -c 0.8}

The \textbf{Memetic Algorithm} will solve the instance \textbf{50\_20\_10} using \textit{population size $= 2$}, with a random seed, maximum time of $300$ seconds, \textit{Transpose mutation} with chance $0.05$, \textit{crossover rate} of $0.8$, and \textit{uniform} weights; no execution trace is written.





% ====================================================
% =============================================== IMPLEMENTATION
% ====================================================


\newpage
\section{Overview of the implementation}
This section will focus on the \textbf{C++} implementation of algorithms to solve \textbf{PFSP}. \\
For each class/file, it is provided a brief description. Note that the $2$ new algorithms were built on top of the existing structure used in the previous assignment.
In this section, the focus will be on new parts that have been added. For a complete overview of the code structure, please refer to the first report. 

$\bullet$ \texttt{flowshop\_iga.cpp}:\\
main access point to \textbf{IGA}. 

$\bullet$ \texttt{flowshop\_gen.cpp}:\\
main access point to the \textbf{Memetic Algorithm}. Using $2$ separate files (and so $2$ executables) was considered the best option as the $2$ algorithm have very different structure and parameters, and keeping a single executable would make things more confusing. 

$\bullet$ \texttt{support\_function.h, support\_function.cpp}: \\
this file provides a series of functions that are used by the optimization algorithms.
These functions are passed as input to the optimization algorithms by using function pointers. \\
The file was updated to contain functions that are used by \textbf{IGA} and by the \textbf{Memetic Algorithm}. Operators for \textit{Destruction/Construction}, \textit{Crossover} and \textit{Mutation} are contained here. Please refer to the algorithm description for more details about them.

$\bullet$ \texttt{iga\_engine.h, iga\_engine.cpp, gen\_engine.h, gen\_engine.cpp}:\\
this class holds the implementation of the \textbf{iterated greedy algorithm} and of the \textbf{memetic algorithm}.\\
Their constructors allows to choose the different parameters, and to specify a \textit{problem} to be solved, or optimized.
The result of the search will be stored inside the class, and accessible to the outside.\\
The algorithms don't directly know how the problem instances or the candidate solutions are implemented, and as such the implementations are very flexible and easily extendible to other problems.\\
The details of the implementations and of the parameters will be given in the next section.

 
% ====================================================
% ====================================ALGORITHM DETAILS
% ====================================================

\section{Details of the implementation}
\subsection{Iterated greedy algorithm}
The Iterated Greedy Algorithm \textbf{(IGA)} applies \textit{local search} to states that are iteratively generated through \textit{heuristic procedures}.\\
At each step, the previous candidate solution is modified through an heuristic procedure called \textit{Destruction/Construction}, and then \textit{Local search} is applied to the result.\\
The proposed implementation is based on the work of \textit{Pan} and \textit{Ruiz} \cite{pan2012local}.

The initial state is generated through the \textbf{RZ heuristic}, already seen in the first assignment.\\
The subsidiary local search is \textbf{Best Improvement} with {Transpose} as neighbourhood function: as seen in the first assignment, this algorithm is able to find good solutions very quickly, as long as the initial state is acceptably good.

The \textbf{Destruction/Construction} procedure that is adopted is the same that is described in the article: a certain number of elements is randomly removed from the solution, and then they are re-added one-by-one in the position that gives the best partial result.\\
As for the number of elements that are removed, $\boldsymbol{10}$, $\boldsymbol{8}$ and $\boldsymbol{6}$ were tested on instances of size $50$, with $10$ being sightly better in terms of solution quality (refer to the specific section for more details).\\
Instances of size $100$ use a value of $8$, as suggested by \textit{Pan} and \textit{Ruiz}.

As explained in \textit{Pan}, \textit{Ruiz} \cite{pan2012local}, candidate solutions that aren't improving are accepted with a probability that is proportional to their quality. Moreover, this probability is a function of the instance size, and of the parameter $\lambda$.\\
Higher values of $\lambda$ will increase the acceptance probability, and make the algorithm more prone to \textit{diversification}. Convergence will be slowed down, however. \\
It was shown that the value of $\lambda$ doesn't have any significant impact on the performances, and as such a value of $\boldsymbol{\lambda = 2}$ was picked, as suggested by the article.

The algorithm stops after a certain amount of time has elapsed; this value was set equal to $500$ times the average execution time of a \textit{Variable Neighbour Descent} algorithm, on similarly sized instances. Concretely, the maximum amount of time on $50$ jobs instances is set to $\boldsymbol{100}$ seconds, while for $100$ jobs instances it is $\boldsymbol{500}$ seconds. \\
However, the algorithm stops if no improvement has been found for a given amount of time, equal to the maximum allowed time divided by $50$.
Using a longer window doesn't seem to provide significant improvements in terms of quality, and the chosen value is a good compromise between execution time and result quality.

\subsection{Memetic algorithm}
\textbf{Memetic algorithms} combine \textit{genetic algorithms} with \textit{local search}. First, a \textbf{crossover} operator is applied to the initial random population.
Then, local search can be applied to the resulting population, followed by a \textbf{mutation} operator. This second population is then improved by using again \textit{local search}, and the resulting candidate solutions will be used as population for the next iteration.

This type of algorithm has a large number of parameters that can be considered. As a starting point, some of the parameters were picked according to \textbf{Bonarini} \href{http://chrome.ws.dei.polimi.it/images/e/eb/GeneticAlgorithms.pdf}{lecture notes, slide 45}.\\
The population size was tested with values of $10$ and $20$, but no significant difference was observed.

\textbf{Crossover} and \textbf{Mutation} are done by sampling candidate solutions from the population. Each candidate solution is picked with a probability that is a function of its fitness. \\
Three different approaches have been tested. In the first, probabilities are simply inversely functional to the fitness; in the second, referred as \texttt{uni10}, we consider the inverse of the $10$-th power of the scores. The idea is to increase the probabilities relative to good solutions, and penalize the others.\\
As extreme case, the third approach is to use the \textit{softmax} function, which is the inverse of the exponential of the scores.  \\
Overall, using a simple uniform probability seems to give the best results, probably because it ensures the highest diversification.

Crossover was applied by using the \textbf{PMX} operator \cite{uccoluk2002genetic}, which makes sure that children candidate solutions are valid permutations.
Pairs of candidate solutions are picked from the population accordingly to their fitness, and are combined through crossover with a probability of $0.99$; lower values were tested, and they gave significantly worse results. 

Mutation is done by swapping $2$ random elements in the candidate solution, with a probability of $0.05$; higher probabilities seems to excessively disrupt good solutions.

In a single iteration of the \textit{memetic algorithm}, \textbf{Best improvement} is applied twice as subsidiary local search procedure: in the first case, after \textit{crossover}, it is used with the \textbf{Transpose} operator, to quickly improve the quality of the population without wasting too much time.\\
After mutation it is applied again with the \textbf{Exchange} operator, which is slower but significantly improves the quality of the population.

The termination criterion is implemented in the same way as \textbf{IGA}.


% ====================================================
% =============================================== TESTS
% ====================================================

\section{Inferential statistical analysis}

\subsection{Introduction}
The implemented algorithms offer a wide choice of parameters to be set.\\
Trying all possible combinations would take a huge amount of time, but it is still possible to perform some tests to check how different parameters can impact the performances, both in terms of \textbf{results} (\textit{weighted completion time})  and \textbf{execution time}.

The algorithms were tested on $60$ different problem instances: $30$ instances with $50$ jobs and $30$ instances with $100$ jobs.
All the instances had $20$ machines. \\
In some tests, where indicated, a smaller subset of instances was used. \\
For each combination of parameters, the same instance has been tested $5$ times, then the \textit{median} execution time and result have been considered. \\
Compared to the \textit{mean}, the \textit{median} is more robust to outliers, and will give more meaningful results.

To have comparable results across the tests on a single problem instance, the \textbf{seed} used by the \textit{random number generator} was kept fixed across a single instance.

\textbf{Note:} contrary to the previous assignment, this time the code has been compiled by using \texttt{-O3} level optimization. To provide results that are comparable, the previous algorithms have also been recompiled with the same optimization level; then, \textbf{VND} has been tested again, with the same procedure used for the 2 new algorithms, in order to have a meaningful comparison.

The tests were performed on the following machine:
\begin{itemize}
    \item Computer: Microsoft Surface Pro 4
    \item CPU: Intel Core i5-6300U at 2.4 GHz (clocked at 2.95 Ghz)
    \item RAM: 4 GB at 1867Mhz
\end{itemize}
 
\newpage 
 
\subsection{Exploratory analysis}
Before starting any statistical test, it is a good idea to visualize the data, so to see if it's immediately possible to notice any interesting structure in the results.

First of all, it is required to separate the data relative to the instances with $50$ jobs from the ones with $100$ jobs, as the values from the 2 sets are not comparable with each other.

Then, it is possible to plot \textbf{boxplots} that display summary statistics of the \textbf{execution times} and of the \textbf{result values}. Relatively to the optimization results, it is also visualized the boxplot of the \textbf{ideal optimization results}, the values that would be achieved by an exact solver.
The results obtained by \textbf{Variable Neighbour Descent} with {Transpose, Insert, Exchange} have been also included in the comparisons, as a reference point.
In the case of $50$ jobs instances, the plots shows also the values for different parameters for \textbf{IGA}. 

\newpage
In the first 2 plots we can compare the execution times of \textbf{IGA}, \textbf{Memetic algorithm} and \textbf{VND} on instances of size $50$ and $100$.
On paper, all the algorithms are given the same maximum execution time, but, as said before, an \textit{early stopping} procedure has been adopted, in order to reduce computation time. As such, it is meaningful to compare the execution times, to see how long it takes to the algorithms to reach a stagnation point.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.2\textwidth, center, keepaspectratio=1]{{"exec_time"}.pdf}
    \caption{\emph{Box plots of the execution times for the various algorithms.}}
\end{figure}

We can see how the new algorithms are significantly slower that \textbf{VND}, and that modifying $8$ or $10$ elements in the \textit{destruction/construction} procedure leads a faster convergence to a local optimum. This can be explained considering that the \textit{destruction/construction} procedure is very fast, and modifying more elements doesn't slow down the algorithm; on the other hand, it improves more the current solution by using the \textit{destruction/construction} heuristic, and causes a faster convergence.

The optimization results are compared by looking at the \textbf{ARPD}, \textit{average percentage relative deviation}, defined as:
$$ ARPD = \frac{1}{R}\sum_{i=1}^{R}{\left(\frac{100 \cdot (S_i - S_{best})}{S_{best}}\right)} $$ 
where $R$ is the number of repetitions of each instance, $S_i$ is the score of a given iteration, and $S_{best}$ is the best known score for the instance (in our case, the optimal solution). Using \textbf{ARPD} prevents the results from being biased by instances with unusually high (or low) scores, and gives more reliable results.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth, center, keepaspectratio=1]{{"arpd"}.pdf}
    \caption{\emph{Summary statistics of the optimization results for the various algorithms.}}
\end{figure}

\textbf{IGA} shows some improvements over \textbf{VND}, at the cost of slower execution time. Note that the population size of the \textbf{Memetic algorithm} is different between instances with $50$ and $100$ jobs, due to computational constraints. This is probably the reason why the algorithm is seemingly better than \textbf{VND} for smaller instances, but worse on the bigger ones.


We can also look more in details at the results of \textbf{IGA}, when $6$, $8$ or $10$ elements are modified in the \textit{destruction/construction} procedure. These tests were done on instances of size $50$. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth, center, keepaspectratio=1]{{"iga_param"}.pdf}
    \caption{\emph{Results for \textbf{IGA} where a different number of elements is modified in \textit{destruction/construction}.}}
\end{figure}

A \textit{Kruskal-Wallis} test was performed to check if there is a statistically significant difference between the 3 values.

\begin{table}[H]
    \centering
    \begin{tabular}{lrrrr}
        \hline
        \\[-1.5ex]
        Type & p.value & Median, 6 & Median, 8 & Median, 10 \\ 
        [0.5ex]
        \hline
        \\[-1.5ex]
        Execution time & 3.142e-05 & 6891.00 & 4615.50 & 5268.00 \\ 
        ARPD & 0.14 & 0.59 & 0.52 & 0.38 \\ 
        \\[-1.6ex]
        \hline
    \end{tabular}
\end{table}

As it is possible to reject the \textit{null hypothesis} that the 3 algorithms have the same execution time, we can perform a further test on the fastest 2, and see if they are different. 

\begin{table}[H]
    \centering
    \begin{tabular}{lrrrr}
        \hline
        \\[-1.5ex]
        Type & p.value & Median, 8 & Median, 10 \\ 
        [0.5ex]
        \hline
        \\[-1.5ex]
        Execution time & 0.3147 & 4615.50 & 5268.00 \\ 
        \\[-1.6ex]
        \hline
    \end{tabular}
\end{table}


It is possible to say with good confidence that using a value of $8$ gives a faster algorithm, but in terms of solution quality the 3 options are pretty much equivalent. It is also not possible to say that using $10$ instead of $8$ will give a faster algorithm, in spite of what the plots might suggest. 
As such, for the remaining part of the report it will be used a value of $8$, accordingly to what was suggested by the article.

We can also compare the results of the \textit{Memetic algorithm} with population size $10$ or $20$.

\begin{table}[H]
    \centering
    \begin{tabular}{lrrrr}
        \hline
        \\[-1.5ex]
        Type & p.value & Median, 10 & Median, 20 \\ 
        [0.5ex]
        \hline
        \\[-1.5ex]
        Execution time & 3.894e-13 & 5174.50 & 7760.50 \\ 
        ARPD & 0.0008 & 2.16 & 1.82 \\ 
        \\[-1.6ex]
        \hline
    \end{tabular}
\end{table}

It can be seen how using a smaller population will give a faster algorithm, but worse results. For instances size of $100$ it was preferred to use a population of $10$, while on instances of $50$ jobs a value of $20$ was used, in order to get the best quality/execution-time \textit{tradeoff}.

\subsection{Summary statistics}

From the collected results, it is possible to compute a number of interesting statistics that will give some additional insight about the performances of the algorithms.\\
For each algorithm, it is possible to compute the \textit{mean}, the \textit{median}, the \textit{standard deviation} (and other statistics) relatively to the execution time and to the optimization results (using \textbf{ARPD}).\\


\begin{table}[H]
    \centering
    \begin{tabular}{rlrrrrr}
        \hline
        \\[-1.5ex]
        Algorithm & Min & Mean & Median & Max & Standard Deviation \\ 
        \hline
        \\[-1.5ex]
        GEN, 20 & 3528.00 & 12028.42 & 7760.50 & 36484.50 & 9926.38 \\ 
        IGA, 8 & 3081.00 & 9562.20 & 4615.50 & 39951.00 & 11517.58 \\ 
        VND, tie & 88.00 & 152.93 & 147.00 & 299.00 & 47.64 \\ 
        \\[-1.6ex]
        \hline
    \end{tabular}
    \caption{\label{tab:50-time}Summary statistics of the \textbf{execution time} of the various algorithms over instances with \textbf{50 jobs}.}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{rlrrrrr}
        \hline
        \\[-1.5ex]
        Algorithm & Min & Mean & Median & Max & Standard Deviation \\ 
        \hline
        \\[-1.5ex]
        GEN, 10 & 30079.00 & 51450.10 & 51113.00 & 67448.00 & 9058.44 \\ 
        IGA, 8  & 18293.00 & 31509.20 & 30382.00 & 50518.00 & 6801.02 \\ 
        VND, tie & 899.00 & 1834.03 & 1882.00 & 3038.00 & 534.49 \\ 
        \\[-1.6ex] 
        \hline
    \end{tabular}
    \caption{\label{tab:100-time}Summary statistics of the \textbf{execution time} of the various algorithms over instances with \textbf{100 jobs}.}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{rlrrrrr}
        \hline
        \\[-1.5ex]
        Algorithm & Min & Mean & Median & Max & Standard Deviation \\ 
        \hline
        \\[-1.5ex]        
        GEN, 20 & 1.32 & 1.91 & 1.82 & 3.13 & 0.42 \\ 
        IGA, 8 & 0.07 & 0.49 & 0.52 & 1.09 & 0.22 \\ 
        VND, tie & 1.11 & 2.07 & 2.05 & 3.10 & 0.50 \\ 
        \\[-1.6ex]
        \hline
    \end{tabular}
    \caption{\label{tab:50-arpd}Summary statistics of the \textbf{ARPD} of the various algorithms over instances with \textbf{50 jobs}.}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{rlrrrrr}
        \hline
        \\[-1.5ex]
        Algorithm & Min & Mean & Median & Max & Standard Deviation \\ 
        \hline
        \\[-1.5ex]
        GEN, 10 & 2.37 & 3.20 & 3.26 & 3.79 & 0.41 \\ 
        IGA, 8 & 0.07 & 0.91 & 0.87 & 1.66 & 0.43 \\ 
        VND, tie & 1.26 & 2.52 & 2.54 & 3.80 & 0.63 \\ 
        \\[-1.6ex]
        \hline
    \end{tabular}
    \caption{\label{tab:100-arpd}Summary statistics of the \textbf{ARPD} of the various algorithms over instances with \textbf{100 jobs}.}
\end{table}   
   
    
The previous tables confirm what was observed with the plots: \textbf{IGA} is the best algorithms in terms of results, but it is a bit slower that \textbf{VND}. The \textbf{Memetic} algorithm is consistently than the other 2 on instances of size $100$.

\subsection{Inferential tests}
It seems obvious that there exist significant differences among the various algorithms; to see if this is actually the case, we will perform statistical tests on the observed results.

Before performing any test, a few considerations about the testing methodology should be made.\\
First, the instances with 50 and 100 jobs should  be treated separately, as they clearly compose different populations.\\
Moreover, the tests to be performed are heavily dependent on the distributions of the populations that are considered.\\
Most tests assume that the samples from a given population are \textit{independent and identically distributed} (\textbf{I.I.D.}): the first condition can be considered true as all the instances are considered separately, without any of them having influences on the others; the second condition is harder to verify, but can be considered true as long as instances with 50 and 100 jobs are considered separately.

If the population aren't normally distributed, it is necessary to resort to \textit{non-parametric} tests, such as the \textbf{Wilcoxon signed-rank test} \cite{wilcoxon1945individual} for comparing the mean of two populations, or the \textbf{Kruskal-Wallis} test to compare more than two populations \cite{kruskal1952use}. The latter test assumes as null hypothesis that all the populations have the same mean, similarly to \textbf{ANOVA}.

\begin{table}[H]
    \centering
    \begin{tabular}{rlrrr}
        \hline
        \\[-1.5ex]
        Type & p.value & Mean, IGA 8 & Mean, GEN 20 \\ 
        \hline
        \\[-1.5ex]
        Execution time & 1.931e-05 & 9562.20 & 12028.42 \\ 
        ARPD & 2.2e-16 & 0.49 & 1.91 \\ 
        \\[-1.6ex]
        \hline
    \end{tabular}
    \caption{\label{tab:p-50}\textbf{p-values} of the \textbf{Wilcoxon} test for the various algorithms, relatively to \textit{execution time} and $ARPD$ of $50$ jobs instances.}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{rlrrr}
        \hline
        \\[-1.5ex]
        Type & p.value & Mean, IGA 8 & Mean, GEN 10 \\ 
        \hline
        \\[-1.5ex]
        Execution time & 1.292e-11 & 31509.20 & 51450.10 \\ 
        ARPD & 2.2e-16 & 0.91 & 3.20 \\ 
        \\[-1.6ex]
        \hline
    \end{tabular}
    \caption{\label{tab:p-100}\textbf{p-values} of the \textbf{Wilcoxon} test for the various algorithms, relatively to \textit{execution time} and $ARPD$ of $100$ jobs instances.}
\end{table}

Unsurprisingly, it is confirmed by the statistical tests that \textbf{IGA} is better than the \textbf{Memetic algorithm} both in \textit{execution time} and in \textit{result quality}.

\subsection{Correlation plot of solution quality}

A useful way to visualize how the 2 algorithms perform with respect to each other is to draw a \textbf{correlation plot}. \\
On the axes, the \textbf{ARPD} (or the \textbf{execution time}), of the 2 algorithms. \\
Each point represents the \textbf{ARPD} (or the \textbf{execution time}) of the 2 algorithms on the same instance. To get more robust results, each point is computed at the mean of 5 different runs on the same instance.

Them, we can compute the correlation coefficient of the 2 algorithms. The correlation is measured as \textbf{Pearson Correlation}. \\
Then, the correlation value is tested to see if it is statistically significant.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.2\textwidth, center, keepaspectratio=1]{{"cor"}.pdf}
    \caption{\emph{Correlation plots of the execution times and of the ARPD for the various algorithms.}}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{rlrrrr}
        \hline
        \\[-1.5ex]
        Type & Correlation & p.value & Mean, IGA & Mean, GEN \\ 
        \hline
        \\[-1.5ex]
        Execution time, 50 & -0.01 & 0.96 & 4550.76 & 7714.12 \\ 
        ARPD, 50 & 0.26 & 0.16 & 0.49 & 1.91 \\ 
        Execution time, 100 & -0.17 & 0.38 & 31509.20 & 51450.10 \\ 
        ARPD, 100 & 0.52 & 0.003 & 0.91 & 3.20 \\ 
        \\[-1.6ex]
        \hline
    \end{tabular}
    \caption{\label{tab:cor}\textbf{Pearson correlation} and \textbf{p-values} of the execution times and of the ARPD for the various algorithms.}
\end{table}

The only case in which there is a statistically significant correlation is for the \textbf{ARPD} of $100$ jobs instances. In all the other cases, it seems that the \textbf{Memetic algorithm} is on average slower, but is more consistent in terms of results and execution time.

\subsection{Run-Time Distributions}
A way to compare the behaviour of different algorithms is to look at the \textbf{run-time distribution}, which can be seen as the function of solution quality over time. More formally, it is a function $P(t):\ \mathbb{R}^+ \rightarrow [0,\ 1]$ such that $P(\bar{t})$ is the probability to find the optimal solution (or some very good solution used as reference point) at time $\bar{t}$.

The run-time distributions were computed by running the first 5 instances with $50$ jobs, $25$ times each. For each of them, the performance over time were recorded by writing the execution trace: to approximate the behaviour of the algorithm over time, it was recorded the solution quality every $100$ steps of the execution. \\
The quality of the approximation can be seen by noticing at how to similar amounts of steps correspond similar elapsed times.\\
Then the values for each instance have been averaged, to produce a smoother curve over time.

The comparison is rather difficult to make, as \textbf{IGA} performs much better than the \textbf{Memetic Algorithm}.\\
It was measured the percentage of times that at a certain time point the solution quality is close than the optimum, where close means that the \textbf{ARPD} is smaller than a certain threshold. \\
The values of \textbf{ARPD} that were considered are $\{0.5\%,  1\%, 2\%\}$.


\begin{figure}[H]
    \subfloat[fig 1]{\includegraphics[width = 3in]{{"rtd0.5"}.pdf}} 
    \subfloat[fig 2]{\includegraphics[width = 3in]{{"rtd1"}.pdf}}\    \subfloat[fig 3]{\includegraphics[width = 3in]{{"rtd1.5"}.pdf}}
    \subfloat[fig 4]{\includegraphics[width = 3in]{{"rtd2"}.pdf}} 
    \caption{Run time distribution with different thresholds. \textbf{IGA} is \textcolor{MidnightBlue}{blue}, \textbf{Memetic} is \textcolor{BrickRed}{red}.}
    \label{rtd}
\end{figure}

We can see once again how \textbf{IGA} works better than the \textbf{Memetic algorithm}. For small \textbf{ARPD} thresholds, the \textbf{Memetic algorithm} often has a probability close to zero of giving solutions better than the threshold, even for high execution times.\\
Only for \textbf{ARPD} $= 2$ the \textbf{Memetic algorithm} seems to be comparable with \textbf{IGA}, and it is even sightly better for high execution times.



\newpage
\bibliographystyle{plainurl}
\bibliography{bibliography}\textbf{}

\end{document}